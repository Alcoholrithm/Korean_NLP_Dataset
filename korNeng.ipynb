{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmiii\\.conda\\envs\\py37\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "kor_tokenizer = get_tokenizer(Hannanum().morphs)\n",
    "eng_tokenizer = get_tokenizer('spacy', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KOR</th>\n",
       "      <th>ENG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>해외에는 세계 최초이자 일본 최대 MCN 기업인 UUUM, 디즈니에 인수된 메이커 ...</td>\n",
       "      <td>Overseas, UUUM, the world's first and the larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>세계 최초의 노인 인권 전담 국제기구가 한국에 들어선다.</td>\n",
       "      <td>The world's first international organization d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>세계 최대 소셜미디어 페이스북 사용자 수억명의 계정 비밀번호(패스워드)가 암호화 장...</td>\n",
       "      <td>\"Account passwords (passwords) of hundreds of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>윤정헌씨에 대한 고문이 30여년 만에야 사실로 확인된 것은 이런 범죄가 얼마나 밝혀...</td>\n",
       "      <td>The fact that the torture of Yoon Jeong-heon w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동락연희단은 앉은 채로 연주했지만, 숨쉴틈 없이 몰아치는 소리에 관객들은 가만히 앉...</td>\n",
       "      <td>The Dongrakyeonhee group performed in a sittin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121687</th>\n",
       "      <td>검찰 측은 경찰 수사에 의문을 품고 보완수사를 요구하더라도 경찰이 “정당하지 못한 ...</td>\n",
       "      <td>The prosecution is questioning the police's in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121688</th>\n",
       "      <td>문을 열자마자 벽에 걸린 진분홍빛 악어가죽이 눈에 들어오고, 진열장 안에서는 저마다...</td>\n",
       "      <td>As soon as you open the door, the pink crocodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121689</th>\n",
       "      <td>우리에게 겨우 일주일밖에 남지 않았습니다.</td>\n",
       "      <td>We only have a week left.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121690</th>\n",
       "      <td>가서 시외버스만 정차하는 버스정류장을 찾으면 되지.</td>\n",
       "      <td>We can go find a bus stop only for intercity b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121691</th>\n",
       "      <td>제가 잘 아는 곳이 있는데 거기로 가실까요.</td>\n",
       "      <td>There's a place I know well, so let's go there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121692 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       KOR  \\\n",
       "0        해외에는 세계 최초이자 일본 최대 MCN 기업인 UUUM, 디즈니에 인수된 메이커 ...   \n",
       "1                          세계 최초의 노인 인권 전담 국제기구가 한국에 들어선다.   \n",
       "2        세계 최대 소셜미디어 페이스북 사용자 수억명의 계정 비밀번호(패스워드)가 암호화 장...   \n",
       "3        윤정헌씨에 대한 고문이 30여년 만에야 사실로 확인된 것은 이런 범죄가 얼마나 밝혀...   \n",
       "4        동락연희단은 앉은 채로 연주했지만, 숨쉴틈 없이 몰아치는 소리에 관객들은 가만히 앉...   \n",
       "...                                                    ...   \n",
       "1121687  검찰 측은 경찰 수사에 의문을 품고 보완수사를 요구하더라도 경찰이 “정당하지 못한 ...   \n",
       "1121688  문을 열자마자 벽에 걸린 진분홍빛 악어가죽이 눈에 들어오고, 진열장 안에서는 저마다...   \n",
       "1121689                            우리에게 겨우 일주일밖에 남지 않았습니다.   \n",
       "1121690                       가서 시외버스만 정차하는 버스정류장을 찾으면 되지.   \n",
       "1121691                           제가 잘 아는 곳이 있는데 거기로 가실까요.   \n",
       "\n",
       "                                                       ENG  \n",
       "0        Overseas, UUUM, the world's first and the larg...  \n",
       "1        The world's first international organization d...  \n",
       "2        \"Account passwords (passwords) of hundreds of ...  \n",
       "3        The fact that the torture of Yoon Jeong-heon w...  \n",
       "4        The Dongrakyeonhee group performed in a sittin...  \n",
       "...                                                    ...  \n",
       "1121687  The prosecution is questioning the police's in...  \n",
       "1121688  As soon as you open the door, the pink crocodi...  \n",
       "1121689                          We only have a week left.  \n",
       "1121690  We can go find a bus stop only for intercity b...  \n",
       "1121691    There's a place I know well, so let's go there.  \n",
       "\n",
       "[1121692 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./datasets/korNeng_corpus/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_iterator(strings, tokenizer):\n",
    "    for string_ in tqdm(strings):\n",
    "        yield tokenizer(string_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_vocab_base = pd.read_csv('./datasets/korNeng_corpus/train_data.csv')['KOR']\n",
    "eng_vocab_base = pd.read_csv('./datasets/korNeng_corpus/train_data.csv')['ENG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1121692/1121692 [37:35<00:00, 497.34it/s]\n",
      "100%|██████████| 1121692/1121692 [00:44<00:00, 24938.89it/s]\n"
     ]
    }
   ],
   "source": [
    "kor_vocab = build_vocab_from_iterator(vocab_iterator(kor_vocab_base, kor_tokenizer), specials=['<unk>', '<pad>', '<bos>', '<eos>'], min_freq=5)\n",
    "eng_vocab = build_vocab_from_iterator(vocab_iterator(eng_vocab_base, eng_tokenizer), specials=['<unk>', '<pad>', '<bos>', '<eos>'], min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_vocab.set_default_index(kor_vocab['<unk>'])\n",
    "eng_vocab.set_default_index(eng_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(kor_vocab, './datasets/korNeng_corpus/kor_vocab.pt')\n",
    "torch.save(eng_vocab, './datasets/korNeng_corpus/eng_vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(file_path):\n",
    "    raw_kor_iter = iter(pd.read_csv(file_path)['KOR'])\n",
    "    raw_eng_iter = iter(pd.read_csv(file_path)['ENG'])\n",
    "    data = []\n",
    "    for (raw_kor, raw_eng) in tqdm(zip(raw_kor_iter, raw_eng_iter)):\n",
    "        kor_tensor_ = torch.tensor([kor_vocab[token] for token in kor_tokenizer(raw_kor)], dtype = torch.long)\n",
    "        eng_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(raw_eng)], dtype = torch.long)\n",
    "\n",
    "        data.append((kor_tensor_, eng_tensor_))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "884994it [49:41, 370.06it/s]"
     ]
    }
   ],
   "source": [
    "train_data = data_process('./datasets/korNeng_corpus/train_data.csv')\n",
    "torch.save(train_data, './datasets/korNeng_corpus/train_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Token 은성유치원은 not found and default index is not set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38472/2382687378.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./datasets/korNeng_corpus/validation_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./datasets/korNeng_corpus/valid_tensor.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38472/635389622.py\u001b[0m in \u001b[0;36mdata_process\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mraw_kor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_eng\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_kor_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_eng_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mkor_tensor_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkor_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkor_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_kor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0meng_tensor_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meng_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meng_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_eng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38472/635389622.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mraw_kor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_eng\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_kor_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_eng_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mkor_tensor_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkor_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkor_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_kor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0meng_tensor_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meng_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meng_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_eng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py37\\lib\\site-packages\\torchtext\\vocab\\vocab.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0massociated\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Token 은성유치원은 not found and default index is not set"
     ]
    }
   ],
   "source": [
    "valid_data = data_process('./datasets/korNeng_corpus/validation_data.csv')\n",
    "torch.save(valid_data, './datasets/korNeng_corpus/valid_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_process('./datasets/korNeng_corpus/test_data.csv')\n",
    "torch.save(test_data, './datasets/korNeng_corpus/test_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shutdown /h"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f7fc97bf5f739df685398a6b375f5a3b22935caa8ea8f31ce823c485c184ce"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
